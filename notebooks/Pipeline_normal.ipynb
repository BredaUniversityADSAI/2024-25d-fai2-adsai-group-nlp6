{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf83fd",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc872f9",
   "metadata": {},
   "source": [
    "This Python script sets up and configures an Azure Machine Learning training pipeline. It authenticates with an Azure ML workspace and develops a training script that preprocesses data, performs grid search with cross-validation on a Random Forest or Logistic Regression model, evaluates model performance with accuracy and F1-score metrics, and logs the results with MLflow. It handles time-based feature formatting, label encoding, scaling, and model assessment (confusion matrices and feature importance). If the new model is an improvement over previous models, it registers the model in Azure ML for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9853df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\kosta\\AppData\\Local\\Temp\\ipykernel_26496\\1528220697.py:25: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  training_script = '''\n",
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading pipeline_scripts (0.02 MBs): 100%|##########| 16500/16500 [00:00<00:00, 243283.96it/s]\n",
      "\u001b[39m\n",
      "\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline submitted: https://ml.azure.com/runs/silver_whistle_nlmtch7gyl?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "Grid Search: Enabled\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, Model\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, Data\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize MLClient\n",
    "credential = InteractiveBrowserCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"0a94de80-6d3b-49f2-b3e9-ec5818862801\",\n",
    "    resource_group_name=\"buas-y2\",\n",
    "    workspace_name=\"NLP6-2025\"\n",
    ")\n",
    "\n",
    "def create_training_script():\n",
    "    os.makedirs(\"pipeline_scripts\", exist_ok=True)\n",
    "    \n",
    "    training_script = '''\n",
    "import pandas as pd \n",
    "import joblib \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from azureml.core import Run, Model\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def find_csv_file(path):\n",
    "    if os.path.isdir(path):\n",
    "        csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in directory: {path}\")\n",
    "        return csv_files[0]\n",
    "    return path\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    \"\"\"Convert time string (HH:MM:SS,fff) to total seconds\"\"\"\n",
    "    try:\n",
    "        hh_mm_ss, millis = time_str.split(',')\n",
    "        h, m, s = hh_mm_ss.split(':')\n",
    "        return float(h) * 3600 + float(m) * 60 + float(s) + (float(millis)/1000)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def preprocess_data(train_path, test_path): \n",
    "    train_file = find_csv_file(train_path)\n",
    "    test_file = find_csv_file(test_path)\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # Identify time columns and convert to seconds\n",
    "    time_cols = [col for col in train_df.columns \n",
    "                if any(x in str(train_df[col].dtype) for x in ['time', 'object'])]\n",
    "    \n",
    "    for col in time_cols:\n",
    "        if train_df[col].astype(str).str.match(r'\\d{2}:\\d{2}:\\d{2},\\d{3}').any():\n",
    "            train_df[col] = train_df[col].astype(str).apply(convert_time_to_seconds)\n",
    "            test_df[col] = test_df[col].astype(str).apply(convert_time_to_seconds)\n",
    "    \n",
    "    target_columns = ['target', 'Target', 'label', 'Label', 'emotion', 'Emotion']\n",
    "    target_col = next((col for col in target_columns if col in train_df.columns), None)\n",
    "    \n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"No target column found. Available columns: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(train_df[target_col])\n",
    "    y_test = label_encoder.transform(test_df[target_col])\n",
    "    \n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    X_test = test_df.drop(columns=[target_col])\n",
    "    \n",
    "    # Convert remaining string columns to numeric or categorical\n",
    "    for col in X_train.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='raise')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='raise')\n",
    "        except:\n",
    "            X_train[col] = LabelEncoder().fit_transform(X_train[col].astype(str))\n",
    "            X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\n",
    "    \n",
    "    # Handle missing values\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(scaler, 'outputs/scaler.pkl')\n",
    "    joblib.dump(label_encoder, 'outputs/label_encoder.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def get_param_grid(model_name):\n",
    "    \"\"\"Define parameter grids for different models\"\"\"\n",
    "    if model_name == \"random_forest\":\n",
    "        return {\n",
    "            'n_estimators': [50, 100, 150, 200],\n",
    "            'max_depth': [10, 15, 20, 25, None],\n",
    "            'min_samples_split': [2, 3, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif model_name == \"logistic_regression\":\n",
    "        return {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'max_iter': [1000, 2000]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "def train_with_grid_search(args):\n",
    "    run = Run.get_context()\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(args.train_data, args.test_data)\n",
    "        \n",
    "        # Model selection\n",
    "        if args.model_name == \"random_forest\":\n",
    "            base_model = RandomForestClassifier(random_state=42)\n",
    "        elif args.model_name == \"logistic_regression\":\n",
    "            base_model = LogisticRegression(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {args.model_name}\")\n",
    "        \n",
    "        # Get parameter grid\n",
    "        param_grid = get_param_grid(args.model_name)\n",
    "        \n",
    "        # Setup Grid Search\n",
    "        cv = StratifiedKFold(n_splits=args.cv_folds, shuffle=True, random_state=42)\n",
    "        scoring = 'f1_weighted'  # Using F1 as primary metric\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=args.n_jobs,\n",
    "            verbose=1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting Grid Search with {len(param_grid)} parameter combinations...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model and predictions\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        best_cv_score = grid_search.best_score_\n",
    "        \n",
    "        preds = best_model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        precision = precision_score(y_test, preds, average=\"weighted\")\n",
    "        recall = recall_score(y_test, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "\n",
    "        os.makedirs(args.model_output, exist_ok=True)\n",
    "        model_path = os.path.join(args.model_output, 'model.pkl')\n",
    "        joblib.dump(best_model, model_path)\n",
    "        \n",
    "        # Save grid search results\n",
    "        grid_results_path = os.path.join(args.model_output, 'grid_search_results.json')\n",
    "        grid_results = {\n",
    "            'best_params': best_params,\n",
    "            'best_cv_score': best_cv_score,\n",
    "            'cv_results': {\n",
    "                'mean_test_score': grid_search.cv_results_['mean_test_score'].tolist(),\n",
    "                'std_test_score': grid_search.cv_results_['std_test_score'].tolist(),\n",
    "                'params': [str(p) for p in grid_search.cv_results_['params']]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(grid_results_path, 'w') as f:\n",
    "            json.dump(grid_results, f, indent=2)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"best_cv_score\", best_cv_score)\n",
    "        mlflow.log_params(best_params)\n",
    "        mlflow.log_param(\"model_name\", args.model_name)\n",
    "        mlflow.log_param(\"cv_folds\", args.cv_folds)\n",
    "        \n",
    "        # Log grid search results as artifact\n",
    "        mlflow.log_artifact(grid_results_path)\n",
    "\n",
    "        # Log F1 vs threshold for models with predict_proba\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            proba = best_model.predict_proba(X_test)\n",
    "            if proba.shape[1] > 1:  # Multi-class\n",
    "                proba = proba[:, 1] if proba.shape[1] == 2 else np.max(proba, axis=1)\n",
    "            else:\n",
    "                proba = proba[:, 0]\n",
    "                \n",
    "            thresholds = np.linspace(0, 1, 50)\n",
    "            for i, thresh in enumerate(thresholds):\n",
    "                preds_thresh = (proba >= thresh).astype(int)\n",
    "                try:\n",
    "                    f1_step = f1_score(y_test, preds_thresh, average=\"weighted\", zero_division=0)\n",
    "                except ValueError:\n",
    "                    f1_step = 0\n",
    "                mlflow.log_metric(\"f1_vs_threshold\", f1_step, step=i)\n",
    "\n",
    "        # Log confusion matrix\n",
    "        cm_display = ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "        plt.title(f\"Confusion Matrix - {args.model_name}\")\n",
    "        conf_matrix_path = os.path.join(args.model_output, \"confusion_matrix.png\")\n",
    "        plt.savefig(conf_matrix_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(conf_matrix_path)\n",
    "\n",
    "        # Log parameter importance plot for Random Forest\n",
    "        if args.model_name == \"random_forest\" and hasattr(best_model, 'feature_importances_'):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            feature_importance = best_model.feature_importances_\n",
    "            indices = np.argsort(feature_importance)[::-1][:20]  # Top 20 features\n",
    "            \n",
    "            plt.title(\"Top 20 Feature Importances\")\n",
    "            plt.bar(range(len(indices)), feature_importance[indices])\n",
    "            plt.xlabel(\"Feature Index\")\n",
    "            plt.ylabel(\"Importance\")\n",
    "            \n",
    "            feature_importance_path = os.path.join(args.model_output, \"feature_importance.png\")\n",
    "            plt.savefig(feature_importance_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(feature_importance_path)\n",
    "\n",
    "        # Model registration logic (same as before)\n",
    "        run.upload_file(name='model/model.pkl', path_or_stream=model_path)\n",
    "        workspace = run.experiment.workspace\n",
    "        models = Model.list(workspace, name=args.model_name)\n",
    "        best_f1_registered = -1.0\n",
    "        for m in models:\n",
    "            try:\n",
    "                f1_val = float(m.properties.get(\"f1_score\", -1.0))\n",
    "                if f1_val > best_f1_registered:\n",
    "                    best_f1_registered = f1_val\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if f1 > best_f1_registered:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            run.register_model(\n",
    "                model_name=args.model_name,\n",
    "                model_path='model/model.pkl',\n",
    "                tags={\n",
    "                    \"framework\": \"sklearn\", \n",
    "                    \"f1_score\": f\"{f1:.4f}\",\n",
    "                    \"best_cv_score\": f\"{best_cv_score:.4f}\",\n",
    "                    \"grid_search\": \"true\"\n",
    "                },\n",
    "                properties={\n",
    "                    \"accuracy\": f\"{acc:.4f}\", \n",
    "                    \"f1_score\": f\"{f1:.4f}\", \n",
    "                    \"best_cv_score\": f\"{best_cv_score:.4f}\",\n",
    "                    \"best_params\": str(best_params),\n",
    "                    \"registered_at_utc\": timestamp\n",
    "                }\n",
    "            )\n",
    "            print(f\"Model registered as {args.model_name} with F1 score: {f1:.4f} (CV: {best_cv_score:.4f})\")\n",
    "        else:\n",
    "            print(f\"Model NOT registered: F1 score {f1:.4f} is not better than previous best {best_f1_registered:.4f}.\")\n",
    "\n",
    "        # Print best parameters\n",
    "        print(f\"\\\\nBest parameters for {args.model_name}:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"Best CV Score: {best_cv_score:.4f}\")\n",
    "        print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "        return acc, best_params, best_cv_score\n",
    "        \n",
    "    finally:\n",
    "        mlflow.end_run()\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Original training function for backward compatibility\"\"\"\n",
    "    run = Run.get_context()\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(args.train_data, args.test_data)\n",
    "        \n",
    "        # Model selection with manual parameters\n",
    "        if args.model_name == \"random_forest\":\n",
    "            params = {\n",
    "                \"n_estimators\": args.n_estimators, \n",
    "                \"max_depth\": args.max_depth,\n",
    "                \"min_samples_split\": args.min_samples_split,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = RandomForestClassifier(**params)\n",
    "        elif args.model_name == \"logistic_regression\":\n",
    "            params = {\n",
    "                \"C\": args.C,\n",
    "                \"max_iter\": 1000,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = LogisticRegression(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {args.model_name}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        precision = precision_score(y_test, preds, average=\"weighted\")\n",
    "        recall = recall_score(y_test, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "\n",
    "        os.makedirs(args.model_output, exist_ok=True)\n",
    "        model_path = os.path.join(args.model_output, 'model.pkl')\n",
    "        joblib.dump(model, model_path)\n",
    "\n",
    "        # Log scalar metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"model_name\", args.model_name)\n",
    "\n",
    "        # Log F1 vs threshold for custom chart\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_test)[:, 1]\n",
    "            thresholds = np.linspace(0, 1, 50)\n",
    "            for i, thresh in enumerate(thresholds):\n",
    "                preds_thresh = (proba >= thresh).astype(int)\n",
    "                try:\n",
    "                    f1_step = f1_score(y_test, preds_thresh, average=\"weighted\", zero_division=0)\n",
    "                except ValueError:\n",
    "                    f1_step = 0\n",
    "                mlflow.log_metric(\"f1_vs_threshold\", f1_step, step=i)\n",
    "\n",
    "        # Log confusion matrix as artifact\n",
    "        cm_display = ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        conf_matrix_path = os.path.join(args.model_output, \"confusion_matrix.png\")\n",
    "        plt.savefig(conf_matrix_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(conf_matrix_path)\n",
    "\n",
    "        # Register model only if F1 is better than previous versions\n",
    "        run.upload_file(name='model/model.pkl', path_or_stream=model_path)\n",
    "        workspace = run.experiment.workspace\n",
    "        models = Model.list(workspace, name=args.model_name)\n",
    "        best_f1_registered = -1.0\n",
    "        for m in models:\n",
    "            try:\n",
    "                f1_val = float(m.properties.get(\"f1_score\", -1.0))\n",
    "                if f1_val > best_f1_registered:\n",
    "                    best_f1_registered = f1_val\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if f1 > best_f1_registered:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            run.register_model(\n",
    "                model_name=args.model_name,\n",
    "                model_path='model/model.pkl',\n",
    "                tags={\"framework\": \"sklearn\", \"f1_score\": f\"{f1:.4f}\"},\n",
    "                properties={\"accuracy\": f\"{acc:.4f}\", \"f1_score\": f\"{f1:.4f}\", \"registered_at_utc\": timestamp}\n",
    "            )\n",
    "            print(f\"Model registered as {args.model_name} with F1 score: {f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"Model NOT registered: F1 score {f1:.4f} is not better than previous best {best_f1_registered:.4f}.\")\n",
    "\n",
    "        return acc\n",
    "    finally:\n",
    "        mlflow.end_run()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_data', type=str, required=True)\n",
    "    parser.add_argument('--test_data', type=str, required=True)\n",
    "    parser.add_argument('--model_output', type=str, required=True)\n",
    "    parser.add_argument('--model_name', type=str, required=True)\n",
    "    \n",
    "    # Grid search parameters\n",
    "    parser.add_argument('--use_grid_search', type=bool, default=True)\n",
    "    parser.add_argument('--cv_folds', type=int, default=5)\n",
    "    parser.add_argument('--n_jobs', type=int, default=-1)\n",
    "    \n",
    "    # Manual parameters (used when grid search is disabled)\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=20)\n",
    "    parser.add_argument('--min_samples_split', type=int, default=2)\n",
    "    parser.add_argument('--C', type=float, default=1.0)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.use_grid_search:\n",
    "        accuracy, best_params, cv_score = train_with_grid_search(args)\n",
    "        print(f\"Grid Search completed. Best CV score: {cv_score:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "    else:\n",
    "        accuracy = train_and_evaluate(args)\n",
    "        print(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    with open(\"pipeline_scripts/train_model.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(training_script)\n",
    "\n",
    "def create_training_component():\n",
    "    create_training_script()\n",
    "    \n",
    "    return command(\n",
    "        name=\"emotion_model_training\",\n",
    "        display_name=\"Emotion Classification Training with Grid Search\",\n",
    "        description=\"Trains emotion classification models with grid search optimization and time feature handling\",\n",
    "        code=\"./pipeline_scripts\",\n",
    "        command=\"python train_model.py \"\n",
    "                \"--train_data ${{inputs.train_data}} \"\n",
    "                \"--test_data ${{inputs.test_data}} \"\n",
    "                \"--model_output ${{outputs.model_output}} \"\n",
    "                \"--model_name ${{inputs.model_name}} \"\n",
    "                \"--use_grid_search ${{inputs.use_grid_search}} \"\n",
    "                \"--cv_folds ${{inputs.cv_folds}} \"\n",
    "                \"--n_jobs ${{inputs.n_jobs}} \"\n",
    "                \"--n_estimators ${{inputs.n_estimators}} \"\n",
    "                \"--max_depth ${{inputs.max_depth}} \"\n",
    "                \"--min_samples_split ${{inputs.min_samples_split}} \"\n",
    "                \"--C ${{inputs.C}}\",\n",
    "        environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "        inputs={\n",
    "            \"train_data\": Input(type=AssetTypes.URI_FOLDER),\n",
    "            \"test_data\": Input(type=AssetTypes.URI_FOLDER),\n",
    "            \"model_name\": Input(type=\"string\"),\n",
    "            \"use_grid_search\": Input(type=\"boolean\", default=True),\n",
    "            \"cv_folds\": Input(type=\"integer\", default=5),\n",
    "            \"n_jobs\": Input(type=\"integer\", default=-1),\n",
    "            # Manual parameters (fallback when grid search is disabled)\n",
    "            \"n_estimators\": Input(type=\"integer\", default=100),\n",
    "            \"max_depth\": Input(type=\"integer\", default=20),\n",
    "            \"min_samples_split\": Input(type=\"integer\", default=2),\n",
    "            \"C\": Input(type=\"number\", default=1.0)\n",
    "        },\n",
    "        outputs={\n",
    "            \"model_output\": Output(type=AssetTypes.URI_FOLDER)\n",
    "        }\n",
    "    )\n",
    "\n",
    "@pipeline()\n",
    "def emotion_training_pipeline(train_data, test_data, use_grid_search=True):\n",
    "    train_component = create_training_component()\n",
    "    \n",
    "    # Random Forest with Grid Search\n",
    "    rf_train = train_component(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        model_name=\"random_forest\",\n",
    "        use_grid_search=use_grid_search,\n",
    "        cv_folds=5,\n",
    "        n_jobs=-1,\n",
    "        # Manual parameters (used only if grid search is disabled)\n",
    "        n_estimators=150,\n",
    "        max_depth=15,\n",
    "        min_samples_split=3\n",
    "    )\n",
    "    rf_train.compute = \"adsai-lambda-0\"\n",
    "    \n",
    "    # Logistic Regression with Grid Search\n",
    "    lr_train = train_component(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        model_name=\"logistic_regression\",\n",
    "        use_grid_search=use_grid_search,\n",
    "        cv_folds=5,\n",
    "        n_jobs=-1,\n",
    "        # Manual parameters (used only if grid search is disabled)\n",
    "        C=1.0\n",
    "    )\n",
    "    lr_train.compute = \"adsai-lambda-0\"\n",
    "    \n",
    "    return {\n",
    "        \"random_forest_output\": rf_train.outputs.model_output,\n",
    "        \"logistic_regression_output\": lr_train.outputs.model_output\n",
    "    }\n",
    "\n",
    "def submit_pipeline(use_grid_search=True):\n",
    "    \"\"\"\n",
    "    Submit the pipeline with optional grid search\n",
    "    \n",
    "    Args:\n",
    "        use_grid_search (bool): Whether to use grid search for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_data = ml_client.data.get(name=\"emotion-raw-train\", label=\"latest\")\n",
    "        test_data = ml_client.data.get(name=\"emotion-raw-test\", label=\"latest\")\n",
    "        \n",
    "        pipeline_job = emotion_training_pipeline(\n",
    "            train_data=Input(type=AssetTypes.URI_FOLDER, path=train_data.path),\n",
    "            test_data=Input(type=AssetTypes.URI_FOLDER, path=test_data.path),\n",
    "            use_grid_search=use_grid_search\n",
    "        )\n",
    "        pipeline_job.settings.default_compute = \"adsai-lambda-0\"\n",
    "        \n",
    "        experiment_name = \"emotion-classification-gridsearch\" if use_grid_search else \"emotion-classification-manual\"\n",
    "        submitted_job = ml_client.jobs.create_or_update(\n",
    "            pipeline_job,\n",
    "            experiment_name=experiment_name\n",
    "        )\n",
    "        \n",
    "        print(f\"Pipeline submitted: {submitted_job.studio_url}\")\n",
    "        print(f\"Grid Search: {'Enabled' if use_grid_search else 'Disabled'}\")\n",
    "        return submitted_job\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error submitting pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "def submit_grid_search_pipeline():\n",
    "    \"\"\"Convenience function to submit pipeline with grid search enabled\"\"\"\n",
    "    return submit_pipeline(use_grid_search=True)\n",
    "\n",
    "def submit_manual_pipeline():\n",
    "    \"\"\"Convenience function to submit pipeline with manual parameters\"\"\"\n",
    "    return submit_pipeline(use_grid_search=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Submit with grid search by default\n",
    "    submit_grid_search_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
