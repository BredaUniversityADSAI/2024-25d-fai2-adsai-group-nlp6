{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf83fd",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5639fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets in workspace:\n",
      "  - emotion-raw-train (version: None, type: uri_folder)\n",
      "  - emotion-raw-test (version: None, type: uri_folder)\n",
      "  - emotion-train-data-v2 (version: None, type: uri_file)\n",
      "  - emotion-test-data-v2 (version: None, type: uri_file)\n",
      "  - emotion-processed-train (version: None, type: uri_file)\n",
      "  - emotion-processed-test (version: None, type: uri_file)\n",
      "  - emotion-encoders (version: None, type: uri_folder)\n",
      "Starting hyperparameter testing with multiple configurations...\n",
      "Submitting 6 training jobs with different hyperparameters...\n",
      "============================================================\n",
      "\n",
      "Submitting job 1/6: random_forest_config_1\n",
      "Training script created successfully with proper encoding\n",
      "Train dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/55dd9664b81809ba9dd646fa4024abd5/feedback_upload_39__hn2s/\n",
      "Test dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/43d17bfd2745a38aac4e0e6ca508aac0/test/\n",
      "Configuration: random_forest_config_1\n",
      "Model: random_forest\n",
      "Parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 'random_forest_config_1' submitted successfully!\n",
      "Monitor at: https://ml.azure.com/runs/dreamy_animal_gqnj2my846?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "[SUCCESS] Successfully submitted: random_forest_config_1\n",
      "\n",
      "Submitting job 2/6: random_forest_config_2\n",
      "Training script created successfully with proper encoding\n",
      "Train dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/55dd9664b81809ba9dd646fa4024abd5/feedback_upload_39__hn2s/\n",
      "Test dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/43d17bfd2745a38aac4e0e6ca508aac0/test/\n",
      "Configuration: random_forest_config_2\n",
      "Model: random_forest\n",
      "Parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5}\n",
      "Job 'random_forest_config_2' submitted successfully!\n",
      "Monitor at: https://ml.azure.com/runs/tidy_candle_h0bqkr5gj5?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "[SUCCESS] Successfully submitted: random_forest_config_2\n",
      "\n",
      "Submitting job 3/6: random_forest_config_3\n",
      "Training script created successfully with proper encoding\n",
      "Train dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/55dd9664b81809ba9dd646fa4024abd5/feedback_upload_39__hn2s/\n",
      "Test dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/43d17bfd2745a38aac4e0e6ca508aac0/test/\n",
      "Configuration: random_forest_config_3\n",
      "Model: random_forest\n",
      "Parameters: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 3}\n",
      "Job 'random_forest_config_3' submitted successfully!\n",
      "Monitor at: https://ml.azure.com/runs/coral_drop_sdgqmrdznh?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "[SUCCESS] Successfully submitted: random_forest_config_3\n",
      "\n",
      "Submitting job 4/6: logistic_regression_config_1\n",
      "Training script created successfully with proper encoding\n",
      "Train dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/55dd9664b81809ba9dd646fa4024abd5/feedback_upload_39__hn2s/\n",
      "Test dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/43d17bfd2745a38aac4e0e6ca508aac0/test/\n",
      "Configuration: logistic_regression_config_1\n",
      "Model: logistic_regression\n",
      "Parameters: {'C': 1.0}\n",
      "Job 'logistic_regression_config_1' submitted successfully!\n",
      "Monitor at: https://ml.azure.com/runs/cool_drop_1rchlp0p18?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "[SUCCESS] Successfully submitted: logistic_regression_config_1\n",
      "\n",
      "Submitting job 5/6: logistic_regression_config_2\n",
      "Training script created successfully with proper encoding\n",
      "Train dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/55dd9664b81809ba9dd646fa4024abd5/feedback_upload_39__hn2s/\n",
      "Test dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/43d17bfd2745a38aac4e0e6ca508aac0/test/\n",
      "Configuration: logistic_regression_config_2\n",
      "Model: logistic_regression\n",
      "Parameters: {'C': 0.1}\n",
      "Job 'logistic_regression_config_2' submitted successfully!\n",
      "Monitor at: https://ml.azure.com/runs/sharp_balloon_w1thry8ktr?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "[SUCCESS] Successfully submitted: logistic_regression_config_2\n",
      "\n",
      "Submitting job 6/6: logistic_regression_config_3\n",
      "Training script created successfully with proper encoding\n",
      "Train dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/55dd9664b81809ba9dd646fa4024abd5/feedback_upload_39__hn2s/\n",
      "Test dataset path: azureml://subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025/datastores/workspaceblobstore/paths/LocalUpload/43d17bfd2745a38aac4e0e6ca508aac0/test/\n",
      "Configuration: logistic_regression_config_3\n",
      "Model: logistic_regression\n",
      "Parameters: {'C': 10.0}\n",
      "Job 'logistic_regression_config_3' submitted successfully!\n",
      "Monitor at: https://ml.azure.com/runs/nifty_boot_0nj23wqwl4?wsid=/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/workspaces/NLP6-2025&tid=0a33589b-0036-4fe8-a829-3ed0926af886\n",
      "[SUCCESS] Successfully submitted: logistic_regression_config_3\n",
      "\n",
      "=== Summary ===\n",
      "Successfully submitted: 6 out of 6 jobs\n",
      "\n",
      "Job Details:\n",
      "  - random_forest_config_1: dreamy_animal_gqnj2my846\n",
      "  - random_forest_config_2: tidy_candle_h0bqkr5gj5\n",
      "  - random_forest_config_3: coral_drop_sdgqmrdznh\n",
      "  - logistic_regression_config_1: cool_drop_1rchlp0p18\n",
      "  - logistic_regression_config_2: sharp_balloon_w1thry8ktr\n",
      "  - logistic_regression_config_3: nifty_boot_0nj23wqwl4\n",
      "\n",
      "============================================================\n",
      "All jobs submitted! You can monitor them in Azure ML Studio.\n",
      "Jobs will automatically register models locally if they pass the threshold.\n",
      "\n",
      "To check job status later, you can use the check_job_status_and_get_models() function.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient, command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.entities import ModelPackage\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Step 1: Connect to Azure ML workspace\n",
    "credential = InteractiveBrowserCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"0a94de80-6d3b-49f2-b3e9-ec5818862801\",\n",
    "    resource_group_name=\"buas-y2\",\n",
    "    workspace_name=\"NLP6-2025\"\n",
    ")\n",
    "\n",
    "# Step 1.5: List available datasets and get details\n",
    "def list_available_datasets():\n",
    "    print(\"Available datasets in workspace:\")\n",
    "    try:\n",
    "        datasets = ml_client.data.list()\n",
    "        dataset_names = []\n",
    "        dataset_info = {}\n",
    "        for dataset in datasets:\n",
    "            dataset_names.append(dataset.name)\n",
    "            dataset_info[dataset.name] = dataset\n",
    "            print(f\"  - {dataset.name} (version: {dataset.version}, type: {dataset.type})\")\n",
    "        return dataset_names, dataset_info\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing datasets: {e}\")\n",
    "        return [], {}\n",
    "\n",
    "# Step 2: Create the enhanced training script with model registration\n",
    "def create_training_script():\n",
    "    os.makedirs(\"azure_scripts\", exist_ok=True)\n",
    "    \n",
    "    # Enhanced training script with model registration capability (Unicode characters removed)\n",
    "    training_script = '''\n",
    "import pandas as pd \n",
    "import joblib \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import time\n",
    "\n",
    "def find_csv_file(path):\n",
    "    \"\"\"Find CSV file in directory or return path if it's a file\"\"\"\n",
    "    if os.path.isdir(path):\n",
    "        csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in directory: {path}\")\n",
    "        print(f\"Found CSV file: {csv_files[0]}\")\n",
    "        return csv_files[0]\n",
    "    return path\n",
    "\n",
    "def preprocess_data(train_path, test_path): \n",
    "    print(\"Loading data...\")\n",
    "    print(f\"Train path: {train_path}\")\n",
    "    print(f\"Test path: {test_path}\")\n",
    "    \n",
    "    # Find CSV files in directories or use direct file paths\n",
    "    train_file = find_csv_file(train_path)\n",
    "    test_file = find_csv_file(test_path)\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    print(\"Dataset Info:\")\n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(\"Column data types:\")\n",
    "    print(train_df.dtypes)\n",
    "    \n",
    "    # Find target column\n",
    "    target_columns = ['target', 'Target', 'label', 'Label', 'emotion', 'Emotion']\n",
    "    target_col = None\n",
    "    for col in target_columns:\n",
    "        if col in train_df.columns:\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"No target column found. Available columns: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    print(f\"Using target column: {target_col}\")\n",
    "    \n",
    "    # Extract and encode target variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(train_df[target_col])\n",
    "    y_test = label_encoder.transform(test_df[target_col])\n",
    "    \n",
    "    print(f\"Unique target values: {label_encoder.classes_}\")\n",
    "    \n",
    "    # Drop target column and process features\n",
    "    X_train = train_df.drop(target_col, axis=1)\n",
    "    X_test = test_df.drop(target_col, axis=1)\n",
    "    \n",
    "    feature_columns = []\n",
    "    for column in X_train.columns:\n",
    "        print(f\"Processing column: {column}\")\n",
    "        try:\n",
    "            # Try numeric conversion\n",
    "            X_train[column] = pd.to_numeric(X_train[column], errors='raise')\n",
    "            X_test[column] = pd.to_numeric(X_test[column], errors='raise')\n",
    "            feature_columns.append(column)\n",
    "            print(f\"Converted to numeric: {column}\")\n",
    "        except (ValueError, TypeError):\n",
    "            # For non-numeric columns, try encoding\n",
    "            try:\n",
    "                label_enc = LabelEncoder()\n",
    "                X_train[column] = label_enc.fit_transform(X_train[column].astype(str))\n",
    "                X_test[column] = label_enc.transform(X_test[column].astype(str))\n",
    "                feature_columns.append(column)\n",
    "                print(f\"Encoded categorical: {column}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping column {column}: {str(e)}\")\n",
    "    \n",
    "    if not feature_columns:\n",
    "        print(\"Column details:\")\n",
    "        for col in X_train.columns:\n",
    "            print(f\"{col}: {X_train[col].dtype}\")\n",
    "            print(f\"Sample values: {X_train[col].head()}\")\n",
    "        raise ValueError(\"No usable features found for training!\")\n",
    "    \n",
    "    X_train = X_train[feature_columns]\n",
    "    X_test = X_test[feature_columns]\n",
    "    \n",
    "    print(f\"Selected features: {feature_columns}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save the scaler and label encoder for model registration\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(scaler, 'outputs/scaler.pkl')\n",
    "    joblib.dump(label_encoder, 'outputs/label_encoder.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def get_model(name, params=None): \n",
    "    if name == \"random_forest\": \n",
    "        return RandomForestClassifier(**(params or {})) \n",
    "    elif name == \"logistic_regression\": \n",
    "        return LogisticRegression(**(params or {})) \n",
    "    else: \n",
    "        raise ValueError(\"Model not supported.\") \n",
    "\n",
    "def log_metrics_to_file(metrics, params):\n",
    "    \"\"\"Log metrics and parameters to files instead of MLflow\"\"\"\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open('outputs/metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Save parameters  \n",
    "    with open('outputs/params.json', 'w') as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    \n",
    "    print(\"Metrics and parameters saved to outputs/\")\n",
    "\n",
    "def safe_mlflow_logging(metrics, params):\n",
    "    \"\"\"Safely attempt MLflow logging with fallback\"\"\"\n",
    "    try:\n",
    "        import mlflow\n",
    "        # Try to set a simple tracking URI to avoid Azure ML registry issues\n",
    "        mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "        \n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "        \n",
    "        for key, value in params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "            \n",
    "        print(\"Successfully logged to MLflow\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"MLflow logging failed: {str(e)}\")\n",
    "        print(\"Falling back to file-based logging...\")\n",
    "        log_metrics_to_file(metrics, params)\n",
    "        return False\n",
    "\n",
    "def register_model_locally(model, accuracy, threshold, model_name, hyperparams):\n",
    "    \"\"\"Register model locally if it passes the threshold\"\"\"\n",
    "    if accuracy >= threshold:\n",
    "        print(f\"Model passed evaluation with accuracy {accuracy:.4f} >= {threshold}\")\n",
    "        print(\"Registering model locally...\")\n",
    "        \n",
    "        # Create model metadata\n",
    "        model_metadata = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"threshold\": threshold,\n",
    "            \"model_type\": model_name,\n",
    "            \"hyperparameters\": hyperparams,\n",
    "            \"training_timestamp\": time.time(),\n",
    "            \"passed_evaluation\": True\n",
    "        }\n",
    "        \n",
    "        # Save model metadata\n",
    "        with open('outputs/model_metadata.json', 'w') as f:\n",
    "            json.dump(model_metadata, f, indent=2)\n",
    "        \n",
    "        # Save the trained model\n",
    "        joblib.dump(model, 'outputs/model.pkl')\n",
    "        \n",
    "        # Create a model registration file\n",
    "        with open('outputs/model_registered.txt', 'w') as f:\n",
    "            f.write(f\"Model registered successfully\\\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy:.4f}\\\\n\")\n",
    "            f.write(f\"Model Type: {model_name}\\\\n\")\n",
    "            f.write(f\"Hyperparameters: {hyperparams}\\\\n\")\n",
    "            f.write(f\"Registration Time: {time.ctime()}\\\\n\")\n",
    "        \n",
    "        print(\"[SUCCESS] Model registered locally in outputs/ directory\")\n",
    "        print(f\"   - Model file: outputs/model.pkl\")\n",
    "        print(f\"   - Metadata: outputs/model_metadata.json\")\n",
    "        print(f\"   - Registration info: outputs/model_registered.txt\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"[FAILED] Model did not pass evaluation: {accuracy:.4f} < {threshold}\")\n",
    "        print(\"Model will not be registered.\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_data', type=str, required=True)\n",
    "    parser.add_argument('--test_data', type=str, required=True)\n",
    "    parser.add_argument('--model_name', type=str, required=True)\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=20)\n",
    "    parser.add_argument('--threshold', type=float, default=0.8)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--min_samples_split', type=int, default=2)\n",
    "    parser.add_argument('--C', type=float, default=1.0)  # For logistic regression\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(args.train_data, args.test_data)\n",
    "    \n",
    "    # Prepare hyperparameters based on model type\n",
    "    if args.model_name == \"random_forest\":\n",
    "        params = {\n",
    "            \"n_estimators\": args.n_estimators, \n",
    "            \"max_depth\": args.max_depth,\n",
    "            \"min_samples_split\": args.min_samples_split,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    elif args.model_name == \"logistic_regression\":\n",
    "        params = {\n",
    "            \"C\": args.C,\n",
    "            \"max_iter\": 1000,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    else:\n",
    "        params = {\"n_estimators\": args.n_estimators, \"max_depth\": args.max_depth}\n",
    "    \n",
    "    model = get_model(args.model_name, params)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    print(f\"Model: {args.model_name}\")\n",
    "    print(f\"Hyperparameters: {params}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    \n",
    "    # Prepare metrics and parameters for logging\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"evaluation_passed\": 1 if acc >= args.threshold else 0\n",
    "    }\n",
    "    \n",
    "    log_params = {\n",
    "        \"model_name\": args.model_name,\n",
    "        \"threshold\": args.threshold,\n",
    "        **params  # Include all model-specific parameters\n",
    "    }\n",
    "    \n",
    "    # Try MLflow logging with fallback\n",
    "    safe_mlflow_logging(metrics, log_params)\n",
    "    \n",
    "    # Register model locally if it passes the threshold\n",
    "    model_registered = register_model_locally(model, acc, args.threshold, args.model_name, params)\n",
    "    \n",
    "    print(f\"\\\\n=== Training Results ===\")\n",
    "    print(f\"Model: {args.model_name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Threshold: {args.threshold}\")\n",
    "    print(f\"Model Registered: {'Yes' if model_registered else 'No'}\")\n",
    "    print(f\"Hyperparameters: {params}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    # Write the script with explicit UTF-8 encoding to avoid charset issues\n",
    "    with open(\"azure_scripts/train_model.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(training_script)\n",
    "    \n",
    "    print(\"Training script created successfully with proper encoding\")\n",
    "\n",
    "# Step 3: Define multiple hyperparameter configurations\n",
    "def get_hyperparameter_configs():\n",
    "    \"\"\"Define different hyperparameter configurations to test\"\"\"\n",
    "    configs = [\n",
    "        {\n",
    "            \"name\": \"random_forest_config_1\",\n",
    "            \"model_name\": \"random_forest\",\n",
    "            \"params\": {\"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 2}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"random_forest_config_2\", \n",
    "            \"model_name\": \"random_forest\",\n",
    "            \"params\": {\"n_estimators\": 200, \"max_depth\": 15, \"min_samples_split\": 5}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"random_forest_config_3\",\n",
    "            \"model_name\": \"random_forest\", \n",
    "            \"params\": {\"n_estimators\": 150, \"max_depth\": 20, \"min_samples_split\": 3}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"logistic_regression_config_1\",\n",
    "            \"model_name\": \"logistic_regression\",\n",
    "            \"params\": {\"C\": 1.0}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"logistic_regression_config_2\",\n",
    "            \"model_name\": \"logistic_regression\", \n",
    "            \"params\": {\"C\": 0.1}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"logistic_regression_config_3\",\n",
    "            \"model_name\": \"logistic_regression\",\n",
    "            \"params\": {\"C\": 10.0}\n",
    "        }\n",
    "    ]\n",
    "    return configs\n",
    "\n",
    "# Step 4: Submit training job with specific hyperparameters\n",
    "def submit_training_job(train_dataset_name, test_dataset_name, config, dataset_info):\n",
    "    from azure.ai.ml import Input\n",
    "    from azure.ai.ml.constants import AssetTypes\n",
    "    \n",
    "    create_training_script()  # Ensure script is created before submitting\n",
    "    \n",
    "    # Get the actual dataset objects with proper version handling\n",
    "    try:\n",
    "        train_dataset = ml_client.data.get(name=train_dataset_name, version=None)\n",
    "        test_dataset = ml_client.data.get(name=test_dataset_name, version=None)\n",
    "    except:\n",
    "        # Fallback: try getting latest version\n",
    "        try:\n",
    "            train_dataset = ml_client.data.get(name=train_dataset_name, label=\"latest\")\n",
    "            test_dataset = ml_client.data.get(name=test_dataset_name, label=\"latest\")\n",
    "        except:\n",
    "            # Last resort: get version 1\n",
    "            train_dataset = ml_client.data.get(name=train_dataset_name, version=\"1\")\n",
    "            test_dataset = ml_client.data.get(name=test_dataset_name, version=\"1\")\n",
    "    \n",
    "    print(f\"Train dataset path: {train_dataset.path}\")\n",
    "    print(f\"Test dataset path: {test_dataset.path}\")\n",
    "    print(f\"Configuration: {config['name']}\")\n",
    "    print(f\"Model: {config['model_name']}\")\n",
    "    print(f\"Parameters: {config['params']}\")\n",
    "\n",
    "    # Determine the correct asset type based on dataset type\n",
    "    train_asset_type = AssetTypes.URI_FOLDER if train_dataset.type == \"uri_folder\" else AssetTypes.URI_FILE\n",
    "    test_asset_type = AssetTypes.URI_FOLDER if test_dataset.type == \"uri_folder\" else AssetTypes.URI_FILE\n",
    "\n",
    "    # Prepare command arguments based on model type\n",
    "    command_args = [\n",
    "        \"python train_model.py\",\n",
    "        \"--train_data ${{inputs.train_data}}\",\n",
    "        \"--test_data ${{inputs.test_data}}\",\n",
    "        \"--model_name ${{inputs.model_name}}\",\n",
    "        \"--threshold 0.8\"\n",
    "    ]\n",
    "    \n",
    "    # Add model-specific parameters\n",
    "    inputs = {\n",
    "        \"train_data\": Input(type=train_asset_type, path=train_dataset.path),\n",
    "        \"test_data\": Input(type=test_asset_type, path=test_dataset.path),\n",
    "        \"model_name\": config[\"model_name\"],\n",
    "    }\n",
    "    \n",
    "    if config[\"model_name\"] == \"random_forest\":\n",
    "        command_args.extend([\n",
    "            \"--n_estimators ${{inputs.n_estimators}}\",\n",
    "            \"--max_depth ${{inputs.max_depth}}\",\n",
    "            \"--min_samples_split ${{inputs.min_samples_split}}\"\n",
    "        ])\n",
    "        inputs.update({\n",
    "            \"n_estimators\": config[\"params\"].get(\"n_estimators\", 100),\n",
    "            \"max_depth\": config[\"params\"].get(\"max_depth\", 20),\n",
    "            \"min_samples_split\": config[\"params\"].get(\"min_samples_split\", 2)\n",
    "        })\n",
    "    elif config[\"model_name\"] == \"logistic_regression\":\n",
    "        command_args.append(\"--C ${{inputs.C}}\")\n",
    "        inputs[\"C\"] = config[\"params\"].get(\"C\", 1.0)\n",
    "\n",
    "    job = command(\n",
    "        code=\"./azure_scripts\",  # directory containing the training script\n",
    "        command=\" \".join(command_args),\n",
    "        environment=\"emotion-clf-pipeline-env:24\",\n",
    "        inputs=inputs,\n",
    "        compute=\"adsai-lambda-0\",\n",
    "        display_name=f\"emotion-training-{config['name']}\",\n",
    "        experiment_name=\"emotion-classification-hyperparameter-testing\"\n",
    "    )\n",
    "\n",
    "    returned_job = ml_client.jobs.create_or_update(job)\n",
    "    print(f\"Job '{config['name']}' submitted successfully!\")\n",
    "    print(f\"Monitor at: {returned_job.studio_url}\")\n",
    "    return returned_job\n",
    "\n",
    "# Step 5: Submit multiple training jobs with different hyperparameters\n",
    "def submit_multiple_training_jobs(train_dataset_name, test_dataset_name, dataset_info):\n",
    "    \"\"\"Submit multiple training jobs with different hyperparameter configurations\"\"\"\n",
    "    configs = get_hyperparameter_configs()\n",
    "    submitted_jobs = []\n",
    "    \n",
    "    print(f\"Submitting {len(configs)} training jobs with different hyperparameters...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, config in enumerate(configs, 1):\n",
    "        print(f\"\\nSubmitting job {i}/{len(configs)}: {config['name']}\")\n",
    "        try:\n",
    "            job = submit_training_job(train_dataset_name, test_dataset_name, config, dataset_info)\n",
    "            submitted_jobs.append({\n",
    "                \"config\": config,\n",
    "                \"job\": job,\n",
    "                \"job_name\": job.name\n",
    "            })\n",
    "            print(f\"[SUCCESS] Successfully submitted: {config['name']}\")\n",
    "            \n",
    "            # Add a small delay between submissions to avoid overwhelming the system\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to submit {config['name']}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n=== Summary ===\")\n",
    "    print(f\"Successfully submitted: {len(submitted_jobs)} out of {len(configs)} jobs\")\n",
    "    print(\"\\nJob Details:\")\n",
    "    for job_info in submitted_jobs:\n",
    "        print(f\"  - {job_info['config']['name']}: {job_info['job_name']}\")\n",
    "    \n",
    "    return submitted_jobs\n",
    "\n",
    "# Step 6: Function to check job status and retrieve successful models\n",
    "def check_job_status_and_get_models(submitted_jobs):\n",
    "    \"\"\"Check the status of submitted jobs and identify successful models\"\"\"\n",
    "    print(\"\\nChecking job statuses...\")\n",
    "    successful_jobs = []\n",
    "    \n",
    "    for job_info in submitted_jobs:\n",
    "        try:\n",
    "            job = ml_client.jobs.get(job_info[\"job_name\"])\n",
    "            status = job.status\n",
    "            print(f\"Job {job_info['config']['name']}: {status}\")\n",
    "            \n",
    "            if status == \"Completed\":\n",
    "                successful_jobs.append(job_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking job {job_info['job_name']}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nCompleted jobs: {len(successful_jobs)}\")\n",
    "    return successful_jobs\n",
    "\n",
    "# Step 7: Main execution with error handling and multiple configurations\n",
    "if __name__ == \"__main__\":\n",
    "    # First, list available datasets\n",
    "    available_datasets, dataset_info = list_available_datasets()\n",
    "    \n",
    "    # Check if the expected datasets exist\n",
    "    expected_train = \"emotion-raw-train\"\n",
    "    expected_test = \"emotion-raw-test\"\n",
    "    \n",
    "    if expected_train not in available_datasets:\n",
    "        print(f\"Dataset '{expected_train}' not found!\")\n",
    "        print(\"Available datasets with 'emotion' in name:\")\n",
    "        emotion_datasets = [d for d in available_datasets if 'emotion' in d.lower()]\n",
    "        for dataset in emotion_datasets:\n",
    "            print(f\"   - {dataset}\")\n",
    "        \n",
    "        if emotion_datasets:\n",
    "            print(f\"\\nSuggestion: Update the dataset names in the script to match available ones.\")\n",
    "        else:\n",
    "            print(\"\\nSuggestion: You may need to register your datasets first.\")\n",
    "            print(\"   Check Azure ML Studio > Data > Datasets to see what's available.\")\n",
    "    else:\n",
    "        # Proceed with multiple job submissions\n",
    "        try:\n",
    "            # Option 1: Submit all hyperparameter configurations\n",
    "            print(\"Starting hyperparameter testing with multiple configurations...\")\n",
    "            submitted_jobs = submit_multiple_training_jobs(\n",
    "                train_dataset_name=expected_train,\n",
    "                test_dataset_name=expected_test,\n",
    "                dataset_info=dataset_info\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"All jobs submitted! You can monitor them in Azure ML Studio.\")\n",
    "            print(\"Jobs will automatically register models locally if they pass the threshold.\")\n",
    "            print(\"\\nTo check job status later, you can use the check_job_status_and_get_models() function.\")\n",
    "            \n",
    "            # Uncomment the following lines if you want to wait and check job status\n",
    "            # print(\"\\nWaiting 5 minutes before checking job status...\")\n",
    "            # time.sleep(300)  # Wait 5 minutes\n",
    "            # successful_jobs = check_job_status_and_get_models(submitted_jobs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main execution: {e}\")\n",
    "            print(\"Check dataset names and environment availability.\")\n",
    "\n",
    "# Additional utility functions\n",
    "def list_hyperparameter_configs():\n",
    "    \"\"\"Display all available hyperparameter configurations\"\"\"\n",
    "    configs = get_hyperparameter_configs()\n",
    "    print(\"Available Hyperparameter Configurations:\")\n",
    "    print(\"=\" * 50)\n",
    "    for config in configs:\n",
    "        print(f\"Name: {config['name']}\")\n",
    "        print(f\"Model: {config['model_name']}\")\n",
    "        print(f\"Parameters: {config['params']}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "def submit_single_config(config_name, train_dataset_name=\"emotion-raw-train\", test_dataset_name=\"emotion-raw-test\"):\n",
    "    \"\"\"Submit a single configuration by name\"\"\"\n",
    "    configs = get_hyperparameter_configs()\n",
    "    selected_config = None\n",
    "    \n",
    "    for config in configs:\n",
    "        if config['name'] == config_name:\n",
    "            selected_config = config\n",
    "            break\n",
    "    \n",
    "    if selected_config is None:\n",
    "        print(f\"Configuration '{config_name}' not found!\")\n",
    "        print(\"Available configurations:\")\n",
    "        for config in configs:\n",
    "            print(f\"  - {config['name']}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        available_datasets, dataset_info = list_available_datasets()\n",
    "        job = submit_training_job(train_dataset_name, test_dataset_name, selected_config, dataset_info)\n",
    "        print(f\"Successfully submitted single configuration: {config_name}\")\n",
    "        return job\n",
    "    except Exception as e:\n",
    "        print(f\"Error submitting single configuration: {e}\")\n",
    "        return None     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-clf-pipeline-2SkgyElo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
