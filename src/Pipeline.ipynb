{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf83fd",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets in workspace:\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, Model\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, Data\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize MLClient\n",
    "credential = InteractiveBrowserCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"0a94de80-6d3b-49f2-b3e9-ec5818862801\",\n",
    "    resource_group_name=\"buas-y2\",\n",
    "    workspace_name=\"NLP6-2025\"\n",
    ")\n",
    "\n",
    "def create_training_script():\n",
    "    os.makedirs(\"pipeline_scripts\", exist_ok=True)\n",
    "    \n",
    "    training_script = '''\n",
    "import pandas as pd \n",
    "import joblib \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import time\n",
    "import mlflow\n",
    "from azureml.core import Run\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def find_csv_file(path):\n",
    "    if os.path.isdir(path):\n",
    "        csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in directory: {path}\")\n",
    "        return csv_files[0]\n",
    "    return path\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    \"\"\"Convert time string (HH:MM:SS,fff) to total seconds\"\"\"\n",
    "    try:\n",
    "        hh_mm_ss, millis = time_str.split(',')\n",
    "        h, m, s = hh_mm_ss.split(':')\n",
    "        return float(h) * 3600 + float(m) * 60 + float(s) + (float(millis)/1000)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def preprocess_data(train_path, test_path): \n",
    "    train_file = find_csv_file(train_path)\n",
    "    test_file = find_csv_file(test_path)\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # Identify time columns and convert to seconds\n",
    "    time_cols = [col for col in train_df.columns \n",
    "                if any(x in str(train_df[col].dtype) for x in ['time', 'object'])]\n",
    "    \n",
    "    for col in time_cols:\n",
    "        if train_df[col].astype(str).str.match(r'\\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{3}').any():\n",
    "            train_df[col] = train_df[col].astype(str).apply(convert_time_to_seconds)\n",
    "            test_df[col] = test_df[col].astype(str).apply(convert_time_to_seconds)\n",
    "    \n",
    "    target_columns = ['target', 'Target', 'label', 'Label', 'emotion', 'Emotion']\n",
    "    target_col = next((col for col in target_columns if col in train_df.columns), None)\n",
    "    \n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"No target column found. Available columns: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(train_df[target_col])\n",
    "    y_test = label_encoder.transform(test_df[target_col])\n",
    "    \n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    X_test = test_df.drop(columns=[target_col])\n",
    "    \n",
    "    # Convert remaining string columns to numeric or categorical\n",
    "    for col in X_train.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='raise')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='raise')\n",
    "        except:\n",
    "            X_train[col] = LabelEncoder().fit_transform(X_train[col].astype(str))\n",
    "            X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\n",
    "    \n",
    "    # Handle missing values\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(scaler, 'outputs/scaler.pkl')\n",
    "    joblib.dump(label_encoder, 'outputs/label_encoder.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    run = Run.get_context()\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(args.train_data, args.test_data)\n",
    "        \n",
    "        if args.model_name == \"random_forest\":\n",
    "            params = {\n",
    "                \"n_estimators\": args.n_estimators, \n",
    "                \"max_depth\": args.max_depth,\n",
    "                \"min_samples_split\": args.min_samples_split,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = RandomForestClassifier(**params)\n",
    "        elif args.model_name == \"logistic_regression\":\n",
    "            params = {\n",
    "                \"C\": args.C,\n",
    "                \"max_iter\": 1000,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = LogisticRegression(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {args.model_name}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        precision = precision_score(y_test, preds, average=\"weighted\")\n",
    "        recall = recall_score(y_test, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "\n",
    "        os.makedirs(args.model_output, exist_ok=True)\n",
    "        joblib.dump(model, os.path.join(args.model_output, 'model.pkl'))\n",
    "\n",
    "        # Log all metrics to MLflow so they show up in AzureML \"Metrics\"\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"model_name\", args.model_name)\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_test)[:, 1]\n",
    "            for i, p in enumerate(proba):\n",
    "                mlflow.log_metric(\"test_positive_proba\", p, step=i)\n",
    "\n",
    "        cm_display = ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        conf_matrix_path = os.path.join(args.model_output, \"confusion_matrix.png\")\n",
    "        plt.savefig(conf_matrix_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(conf_matrix_path)\n",
    "        # Register the model with AzureML\n",
    "        run = Run.get_context()\n",
    "        model_path = os.path.join(args.model_output, 'model.pkl')\n",
    "        run.upload_file(name='model/model.pkl', path_or_stream=model_path)\n",
    "        run.register_model(\n",
    "            model_name=args.model_name,\n",
    "            model_path='model/model.pkl',\n",
    "            tags={\"framework\": \"sklearn\"},\n",
    "            properties={\"accuracy\": acc}\n",
    "        )\n",
    "\n",
    "        return acc\n",
    "        \n",
    "    finally:\n",
    "        mlflow.end_run()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_data', type=str, required=True)\n",
    "    parser.add_argument('--test_data', type=str, required=True)\n",
    "    parser.add_argument('--model_output', type=str, required=True)\n",
    "    parser.add_argument('--model_name', type=str, required=True)\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=20)\n",
    "    parser.add_argument('--min_samples_split', type=int, default=2)\n",
    "    parser.add_argument('--C', type=float, default=1.0)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    accuracy = train_and_evaluate(args)\n",
    "    print(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    with open(\"pipeline_scripts/train_model.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(training_script)\n",
    "\n",
    "def create_training_component():\n",
    "    create_training_script()\n",
    "    \n",
    "    return command(\n",
    "        name=\"emotion_model_training\",\n",
    "        display_name=\"Emotion Classification Training\",\n",
    "        description=\"Trains emotion classification models with time feature handling\",\n",
    "        code=\"./pipeline_scripts\",\n",
    "        command=\"python train_model.py \"\n",
    "                \"--train_data ${{inputs.train_data}} \"\n",
    "                \"--test_data ${{inputs.test_data}} \"\n",
    "                \"--model_output ${{outputs.model_output}} \"\n",
    "                \"--model_name ${{inputs.model_name}} \"\n",
    "                \"--n_estimators ${{inputs.n_estimators}} \"\n",
    "                \"--max_depth ${{inputs.max_depth}} \"\n",
    "                \"--min_samples_split ${{inputs.min_samples_split}} \"\n",
    "                \"--C ${{inputs.C}}\",\n",
    "        environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "        inputs={\n",
    "            \"train_data\": Input(type=AssetTypes.URI_FOLDER),\n",
    "            \"test_data\": Input(type=AssetTypes.URI_FOLDER),\n",
    "            \"model_name\": Input(type=\"string\"),\n",
    "            \"n_estimators\": Input(type=\"integer\", default=100),\n",
    "            \"max_depth\": Input(type=\"integer\", default=20),\n",
    "            \"min_samples_split\": Input(type=\"integer\", default=2),\n",
    "            \"C\": Input(type=\"number\", default=1.0)\n",
    "        },\n",
    "        outputs={\n",
    "            \"model_output\": Output(type=AssetTypes.URI_FOLDER)\n",
    "        }\n",
    "    )\n",
    "\n",
    "@pipeline()\n",
    "def emotion_training_pipeline(train_data, test_data):\n",
    "    train_component = create_training_component()\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_train = train_component(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        model_name=\"random_forest\",\n",
    "        n_estimators=150,\n",
    "        max_depth=15,\n",
    "        min_samples_split=3\n",
    "    )\n",
    "    rf_train.compute = \"adsai-lambda-0\"\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_train = train_component(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        model_name=\"logistic_regression\",\n",
    "        C=1.0\n",
    "    )\n",
    "    lr_train.compute = \"adsai-lambda-0\"\n",
    "    \n",
    "    return {\n",
    "        \"random_forest_output\": rf_train.outputs.model_output,\n",
    "        \"logistic_regression_output\": lr_train.outputs.model_output\n",
    "    }\n",
    "\n",
    "def submit_pipeline():\n",
    "    try:\n",
    "        train_data = ml_client.data.get(name=\"emotion-raw-train\", label=\"latest\")\n",
    "        test_data = ml_client.data.get(name=\"emotion-raw-test\", label=\"latest\")\n",
    "        \n",
    "        pipeline_job = emotion_training_pipeline(\n",
    "            train_data=Input(type=AssetTypes.URI_FOLDER, path=train_data.path),\n",
    "            test_data=Input(type=AssetTypes.URI_FOLDER, path=test_data.path)\n",
    "        )\n",
    "        pipeline_job.settings.default_compute = \"adsai-lambda-0\"\n",
    "        \n",
    "        submitted_job = ml_client.jobs.create_or_update(\n",
    "            pipeline_job,\n",
    "            experiment_name=\"emotion-classification-v2\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Pipeline submitted: {submitted_job.studio_url}\")\n",
    "        return submitted_job\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error submitting pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submit_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae3512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AZURE ML PIPELINE TRAINING SYSTEM ===\n",
      "\n",
      "This script provides two approaches:\n",
      "1. Pipeline-based training (recommended)\n",
      "2. Parallel individual jobs (fallback)\n",
      "=== PIPELINE-BASED TRAINING ===\n",
      "Available datasets in workspace:\n",
      "  - emotion-raw-train (version: None, type: uri_folder)\n",
      "  - emotion-raw-test (version: None, type: uri_folder)\n",
      "  - emotion-train-data-v2 (version: None, type: uri_file)\n",
      "  - emotion-test-data-v2 (version: None, type: uri_file)\n",
      "  - emotion-processed-train (version: None, type: uri_file)\n",
      "  - emotion-processed-test (version: None, type: uri_file)\n",
      "  - emotion-encoders (version: None, type: uri_folder)\n",
      "\n",
      "Available datasets:\n",
      "  - emotion-raw-train\n",
      "  - emotion-raw-test\n",
      "  - emotion-train-data-v2\n",
      "  - emotion-test-data-v2\n",
      "  - emotion-processed-train\n",
      "  - emotion-processed-test\n",
      "  - emotion-encoders\n",
      "Train dataset: emotion-processed-train (version: 17)\n",
      "Test dataset: emotion-processed-test (version: 17)\n",
      "Creating new environment...\n",
      "Error submitting pipeline: No such file or directory: C:\\Users\\noahi\\Desktop\\2024-25d-fai2-adsai-group-nlp6\\src\\azure_scripts\\conda_env.yml\n",
      "Trying alternative parallel job submission...\n",
      "=== PARALLEL JOBS TRAINING ===\n",
      "Available datasets in workspace:\n",
      "  - emotion-raw-train (version: None, type: uri_folder)\n",
      "  - emotion-raw-test (version: None, type: uri_folder)\n",
      "  - emotion-train-data-v2 (version: None, type: uri_file)\n",
      "  - emotion-test-data-v2 (version: None, type: uri_file)\n",
      "  - emotion-processed-train (version: None, type: uri_file)\n",
      "  - emotion-processed-test (version: None, type: uri_file)\n",
      "  - emotion-encoders (version: None, type: uri_folder)\n",
      "\n",
      "Available datasets:\n",
      "  - emotion-raw-train\n",
      "  - emotion-raw-test\n",
      "  - emotion-train-data-v2\n",
      "  - emotion-test-data-v2\n",
      "  - emotion-processed-train\n",
      "  - emotion-processed-test\n",
      "  - emotion-encoders\n",
      "Creating new environment...\n",
      "Error submitting parallel jobs: No such file or directory: C:\\Users\\noahi\\Desktop\\2024-25d-fai2-adsai-group-nlp6\\src\\azure_scripts\\conda_env.yml\n",
      "=== ANALYZING RESULTS FROM EXPERIMENT: emotion-clf-pipeline-env ===\n",
      "Error analyzing experiment results: Session.request() got an unexpected keyword argument 'experiment_name'\n",
      "=== FINDING BEST MODEL FROM EXPERIMENT: emotion-clf-pipeline-env ===\n",
      "=== ANALYZING RESULTS FROM EXPERIMENT: emotion-clf-pipeline-env ===\n",
      "Error analyzing experiment results: Session.request() got an unexpected keyword argument 'experiment_name'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5267750b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moperators\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moperators\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BashOperator\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, Model\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, Data\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.exceptions import AirflowException\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "from airflow.models import Variable\n",
    "import logging\n",
    "\n",
    "# Initialize MLClient\n",
    "credential = InteractiveBrowserCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"0a94de80-6d3b-49f2-b3e9-ec5818862801\",\n",
    "    resource_group_name=\"buas-y2\",\n",
    "    workspace_name=\"NLP6-2025\"\n",
    ")\n",
    "\n",
    "def create_training_script():\n",
    "    os.makedirs(\"pipeline_scripts\", exist_ok=True)\n",
    "    \n",
    "    training_script = '''\n",
    "import pandas as pd \n",
    "import joblib \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import time\n",
    "import mlflow\n",
    "from azureml.core import Run\n",
    "import numpy as np\n",
    "\n",
    "def find_csv_file(path):\n",
    "    if os.path.isdir(path):\n",
    "        csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in directory: {path}\")\n",
    "        return csv_files[0]\n",
    "    return path\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    \"\"\"Convert time string (HH:MM:SS,fff) to total seconds\"\"\"\n",
    "    try:\n",
    "        hh_mm_ss, millis = time_str.split(',')\n",
    "        h, m, s = hh_mm_ss.split(':')\n",
    "        return float(h) * 3600 + float(m) * 60 + float(s) + (float(millis)/1000)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def preprocess_data(train_path, test_path): \n",
    "    train_file = find_csv_file(train_path)\n",
    "    test_file = find_csv_file(test_path)\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # Identify time columns and convert to seconds\n",
    "    time_cols = [col for col in train_df.columns \n",
    "                if any(x in str(train_df[col].dtype) for x in ['time', 'object'])]\n",
    "    \n",
    "    for col in time_cols:\n",
    "        if train_df[col].astype(str).str.match(r'\\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{3}').any():\n",
    "            train_df[col] = train_df[col].astype(str).apply(convert_time_to_seconds)\n",
    "            test_df[col] = test_df[col].astype(str).apply(convert_time_to_seconds)\n",
    "    \n",
    "    target_columns = ['target', 'Target', 'label', 'Label', 'emotion', 'Emotion']\n",
    "    target_col = next((col for col in target_columns if col in train_df.columns), None)\n",
    "    \n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"No target column found. Available columns: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(train_df[target_col])\n",
    "    y_test = label_encoder.transform(test_df[target_col])\n",
    "    \n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    X_test = test_df.drop(columns=[target_col])\n",
    "    \n",
    "    # Convert remaining string columns to numeric or categorical\n",
    "    for col in X_train.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='raise')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='raise')\n",
    "        except:\n",
    "            X_train[col] = LabelEncoder().fit_transform(X_train[col].astype(str))\n",
    "            X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\n",
    "    \n",
    "    # Handle missing values\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(scaler, 'outputs/scaler.pkl')\n",
    "    joblib.dump(label_encoder, 'outputs/label_encoder.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    run = Run.get_context()\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(args.train_data, args.test_data)\n",
    "        \n",
    "        if args.model_name == \"random_forest\":\n",
    "            params = {\n",
    "                \"n_estimators\": args.n_estimators, \n",
    "                \"max_depth\": args.max_depth,\n",
    "                \"min_samples_split\": args.min_samples_split,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = RandomForestClassifier(**params)\n",
    "        elif args.model_name == \"logistic_regression\":\n",
    "            params = {\n",
    "                \"C\": args.C,\n",
    "                \"max_iter\": 1000,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = LogisticRegression(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {args.model_name}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        \n",
    "        os.makedirs(args.model_output, exist_ok=True)\n",
    "        joblib.dump(model, os.path.join(args.model_output, 'model.pkl'))\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"model_name\", args.model_name)\n",
    "        \n",
    "        return acc\n",
    "        \n",
    "    finally:\n",
    "        mlflow.end_run()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_data', type=str, required=True)\n",
    "    parser.add_argument('--test_data', type=str, required=True)\n",
    "    parser.add_argument('--model_output', type=str, required=True)\n",
    "    parser.add_argument('--model_name', type=str, required=True)\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=20)\n",
    "    parser.add_argument('--min_samples_split', type=int, default=2)\n",
    "    parser.add_argument('--C', type=float, default=1.0)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    accuracy = train_and_evaluate(args)\n",
    "    print(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    with open(\"pipeline_scripts/train_model.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(training_script)\n",
    "\n",
    "def create_training_component():\n",
    "    create_training_script()\n",
    "    \n",
    "    return command(\n",
    "        name=\"emotion_model_training\",\n",
    "        display_name=\"Emotion Classification Training\",\n",
    "        description=\"Trains emotion classification models with time feature handling\",\n",
    "        code=\"./pipeline_scripts\",\n",
    "        command=\"python train_model.py \"\n",
    "                \"--train_data ${{inputs.train_data}} \"\n",
    "                \"--test_data ${{inputs.test_data}} \"\n",
    "                \"--model_output ${{outputs.model_output}} \"\n",
    "                \"--model_name ${{inputs.model_name}} \"\n",
    "                \"--n_estimators ${{inputs.n_estimators}} \"\n",
    "                \"--max_depth ${{inputs.max_depth}} \"\n",
    "                \"--min_samples_split ${{inputs.min_samples_split}} \"\n",
    "                \"--C ${{inputs.C}}\",\n",
    "        environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "        inputs={\n",
    "            \"train_data\": Input(type=AssetTypes.URI_FOLDER),\n",
    "            \"test_data\": Input(type=AssetTypes.URI_FOLDER),\n",
    "            \"model_name\": Input(type=\"string\"),\n",
    "            \"n_estimators\": Input(type=\"integer\", default=100),\n",
    "            \"max_depth\": Input(type=\"integer\", default=20),\n",
    "            \"min_samples_split\": Input(type=\"integer\", default=2),\n",
    "            \"C\": Input(type=\"number\", default=1.0)\n",
    "        },\n",
    "        outputs={\n",
    "            \"model_output\": Output(type=AssetTypes.URI_FOLDER)\n",
    "        }\n",
    "    )\n",
    "\n",
    "@pipeline()\n",
    "def emotion_training_pipeline(train_data, test_data):\n",
    "    train_component = create_training_component()\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_train = train_component(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        model_name=\"random_forest\",\n",
    "        n_estimators=150,\n",
    "        max_depth=15,\n",
    "        min_samples_split=3\n",
    "    )\n",
    "    rf_train.compute = \"adsai-lambda-0\"\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_train = train_component(\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        model_name=\"logistic_regression\",\n",
    "        C=1.0\n",
    "    )\n",
    "    lr_train.compute = \"adsai-lambda-0\"\n",
    "    \n",
    "    return {\n",
    "        \"random_forest_output\": rf_train.outputs.model_output,\n",
    "        \"logistic_regression_output\": lr_train.outputs.model_output\n",
    "    }\n",
    "\n",
    "# Airflow Task Functions\n",
    "def validate_data_availability(**kwargs):\n",
    "    \"\"\"Check if data assets are available before starting pipeline\"\"\"\n",
    "    try:\n",
    "        train_data = ml_client.data.get(name=\"emotion-raw-train\", label=\"latest\")\n",
    "        test_data = ml_client.data.get(name=\"emotion-raw-test\", label=\"latest\")\n",
    "        \n",
    "        logging.info(f\"Train data found: {train_data.name} v{train_data.version}\")\n",
    "        logging.info(f\"Test data found: {test_data.name} v{test_data.version}\")\n",
    "        \n",
    "        ti = kwargs['ti']\n",
    "        ti.xcom_push(key='train_data_path', value=train_data.path)\n",
    "        ti.xcom_push(key='test_data_path', value=test_data.path)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Data validation failed: {e}\")\n",
    "        raise AirflowException(f\"Required data assets not found: {e}\")\n",
    "\n",
    "def submit_pipeline(**kwargs):\n",
    "    try:\n",
    "        ti = kwargs['ti']\n",
    "        \n",
    "        # Get data paths from previous task\n",
    "        train_data_path = ti.xcom_pull(task_ids='validate_data', key='train_data_path')\n",
    "        test_data_path = ti.xcom_pull(task_ids='validate_data', key='test_data_path')\n",
    "        \n",
    "        if not train_data_path or not test_data_path:\n",
    "            # Fallback to direct data retrieval\n",
    "            train_data = ml_client.data.get(name=\"emotion-raw-train\", label=\"latest\")\n",
    "            test_data = ml_client.data.get(name=\"emotion-raw-test\", label=\"latest\")\n",
    "            train_data_path = train_data.path\n",
    "            test_data_path = test_data.path\n",
    "        \n",
    "        pipeline_job = emotion_training_pipeline(\n",
    "            train_data=Input(type=AssetTypes.URI_FOLDER, path=train_data_path),\n",
    "            test_data=Input(type=AssetTypes.URI_FOLDER, path=test_data_path)\n",
    "        )\n",
    "        pipeline_job.settings.default_compute = \"adsai-lambda-0\"\n",
    "        \n",
    "        submitted_job = ml_client.jobs.create_or_update(\n",
    "            pipeline_job,\n",
    "            experiment_name=\"emotion-classification-v2\"\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Pipeline submitted: {submitted_job.studio_url}\")\n",
    "        \n",
    "        # Push job details to XCom\n",
    "        ti.xcom_push(key='job_id', value=submitted_job.id)\n",
    "        ti.xcom_push(key='job_url', value=submitted_job.studio_url)\n",
    "        ti.xcom_push(key='experiment_name', value=\"emotion-classification-v2\")\n",
    "        \n",
    "        return submitted_job.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error submitting pipeline: {e}\")\n",
    "        raise AirflowException(f\"Pipeline submission failed: {e}\")\n",
    "\n",
    "def monitor_pipeline(**kwargs):\n",
    "    try:\n",
    "        ti = kwargs['ti']\n",
    "        job_id = ti.xcom_pull(task_ids='ml_pipeline_group.submit_pipeline', key='job_id')\n",
    "        \n",
    "        if not job_id:\n",
    "            raise AirflowException(\"No job ID found from previous task\")\n",
    "        \n",
    "        job = ml_client.jobs.get(job_id)\n",
    "        \n",
    "        logging.info(f\"Pipeline status: {job.status}\")\n",
    "        \n",
    "        # Push status to XCom for downstream tasks\n",
    "        ti.xcom_push(key='job_status', value=job.status)\n",
    "        ti.xcom_push(key='job_details', value={\n",
    "            'id': job.id,\n",
    "            'status': job.status,\n",
    "            'creation_time': str(job.creation_context.created_at) if job.creation_context else None\n",
    "        })\n",
    "        \n",
    "        if job.status in [\"Completed\", \"Finished\"]:\n",
    "            logging.info(f\"Pipeline completed successfully: {job.studio_url}\")\n",
    "            return True\n",
    "        elif job.status in [\"Failed\", \"Canceled\"]:\n",
    "            raise AirflowException(f\"Pipeline failed with status: {job.status}\")\n",
    "        else:\n",
    "            logging.info(f\"Pipeline still running. Current status: {job.status}\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error monitoring pipeline: {e}\")\n",
    "        raise AirflowException(f\"Pipeline monitoring failed: {e}\")\n",
    "\n",
    "def collect_metrics(**kwargs):\n",
    "    \"\"\"Collect and log metrics from completed pipeline\"\"\"\n",
    "    try:\n",
    "        ti = kwargs['ti']\n",
    "        job_id = ti.xcom_pull(task_ids='ml_pipeline_group.submit_pipeline', key='job_id')\n",
    "        \n",
    "        if not job_id:\n",
    "            logging.warning(\"No job ID found, skipping metrics collection\")\n",
    "            return\n",
    "        \n",
    "        job = ml_client.jobs.get(job_id)\n",
    "        \n",
    "        # Log pipeline completion metrics\n",
    "        logging.info(f\"Pipeline {job_id} completed\")\n",
    "        logging.info(f\"Experiment: {job.experiment_name}\")\n",
    "        logging.info(f\"Total duration: {job.creation_context}\")\n",
    "        \n",
    "        # Store metrics in XCom for potential downstream use\n",
    "        ti.xcom_push(key='pipeline_metrics', value={\n",
    "            'job_id': job_id,\n",
    "            'status': job.status,\n",
    "            'experiment_name': job.experiment_name,\n",
    "            'completion_time': str(datetime.now())\n",
    "        })\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error collecting metrics: {e}\")\n",
    "        # Don't fail the pipeline for metrics collection issues\n",
    "        return False\n",
    "\n",
    "def send_completion_notification(**kwargs):\n",
    "    \"\"\"Send notification about pipeline completion\"\"\"\n",
    "    try:\n",
    "        ti = kwargs['ti']\n",
    "        job_url = ti.xcom_pull(task_ids='ml_pipeline_group.submit_pipeline', key='job_url')\n",
    "        job_status = ti.xcom_pull(task_ids='ml_pipeline_group.monitor_pipeline', key='job_status')\n",
    "        \n",
    "        message = f\"\"\"\n",
    "        Azure ML Pipeline Completed Successfully!\n",
    "        \n",
    "        Status: {job_status}\n",
    "        Studio URL: {job_url}\n",
    "        Completion Time: {datetime.now()}\n",
    "        \n",
    "        The emotion classification models have been trained and are ready for use.\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(\"Pipeline completion notification prepared\")\n",
    "        logging.info(message)\n",
    "        \n",
    "        # In a real environment, you might send this via email, Slack, etc.\n",
    "        return message\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error sending notification: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanup_resources(**kwargs):\n",
    "    \"\"\"Clean up temporary resources if needed\"\"\"\n",
    "    try:\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(\"pipeline_scripts\"):\n",
    "            logging.info(\"Cleaning up pipeline scripts directory\")\n",
    "        \n",
    "        logging.info(\"Resource cleanup completed\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during cleanup: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_airflow_dag():\n",
    "    default_args = {\n",
    "        'owner': 'data-science-team',\n",
    "        'depends_on_past': False,\n",
    "        'email_on_failure': True,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 2,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        'execution_timeout': timedelta(hours=2),\n",
    "    }\n",
    "\n",
    "    dag = DAG(\n",
    "        'azureml_emotion_classification_enhanced',\n",
    "        default_args=default_args,\n",
    "        description='Enhanced orchestration of Azure ML Emotion Classification Pipeline',\n",
    "        schedule_interval='@weekly',  # Run weekly\n",
    "        start_date=datetime(2024, 1, 1),\n",
    "        catchup=False,\n",
    "        tags=['azureml', 'emotion-classification', 'ml-pipeline'],\n",
    "        max_active_runs=1,  # Prevent concurrent runs\n",
    "    )\n",
    "\n",
    "    # Pre-pipeline validation task\n",
    "    validate_data_task = PythonOperator(\n",
    "        task_id='validate_data',\n",
    "        python_callable=validate_data_availability,\n",
    "        provide_context=True,\n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "    # Create a task group for ML pipeline tasks\n",
    "    with TaskGroup(\"ml_pipeline_group\", dag=dag) as ml_pipeline_group:\n",
    "        \n",
    "        submit_task = PythonOperator(\n",
    "            task_id='submit_pipeline',\n",
    "            python_callable=submit_pipeline,\n",
    "            provide_context=True,\n",
    "        )\n",
    "\n",
    "        monitor_task = PythonOperator(\n",
    "            task_id='monitor_pipeline',\n",
    "            python_callable=monitor_pipeline,\n",
    "            provide_context=True,\n",
    "            retries=12,  # Will retry for up to 1 hour (5 min * 12)\n",
    "            retry_delay=timedelta(minutes=5),\n",
    "            poke_interval=300,  # Check every 5 minutes\n",
    "        )\n",
    "\n",
    "        submit_task >> monitor_task\n",
    "\n",
    "    # Post-pipeline tasks\n",
    "    collect_metrics_task = PythonOperator(\n",
    "        task_id='collect_metrics',\n",
    "        python_callable=collect_metrics,\n",
    "        provide_context=True,\n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "    send_notification_task = PythonOperator(\n",
    "        task_id='send_notification',\n",
    "        python_callable=send_completion_notification,\n",
    "        provide_context=True,\n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "    cleanup_task = PythonOperator(\n",
    "        task_id='cleanup_resources',\n",
    "        python_callable=cleanup_resources,\n",
    "        provide_context=True,\n",
    "        trigger_rule='all_done',  # Run regardless of upstream success/failure\n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "    # Define task dependencies\n",
    "    validate_data_task >> ml_pipeline_group >> [collect_metrics_task, send_notification_task] >> cleanup_task\n",
    "\n",
    "    return dag\n",
    "\n",
    "# Create the enhanced DAG\n",
    "emotion_classification_dag = create_airflow_dag()\n",
    "\n",
    "# Alternative DAG for manual/on-demand runs\n",
    "def create_manual_dag():\n",
    "    \"\"\"Create a separate DAG for manual pipeline runs\"\"\"\n",
    "    default_args = {\n",
    "        'owner': 'data-science-team',\n",
    "        'depends_on_past': False,\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=2),\n",
    "    }\n",
    "\n",
    "    manual_dag = DAG(\n",
    "        'azureml_emotion_classification_manual',\n",
    "        default_args=default_args,\n",
    "        description='Manual trigger for Azure ML Emotion Classification Pipeline',\n",
    "        schedule_interval=None,  # Manual trigger only\n",
    "        start_date=datetime(2024, 1, 1),\n",
    "        catchup=False,\n",
    "        tags=['azureml', 'emotion-classification', 'manual'],\n",
    "    )\n",
    "\n",
    "    manual_submit = PythonOperator(\n",
    "        task_id='manual_submit_pipeline',\n",
    "        python_callable=submit_pipeline,\n",
    "        provide_context=True,\n",
    "        dag=manual_dag,\n",
    "    )\n",
    "\n",
    "    manual_monitor = PythonOperator(\n",
    "        task_id='manual_monitor_pipeline',\n",
    "        python_callable=monitor_pipeline,\n",
    "        provide_context=True,\n",
    "        retries=6,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        dag=manual_dag,\n",
    "    )\n",
    "\n",
    "    manual_submit >> manual_monitor\n",
    "\n",
    "    return manual_dag\n",
    "\n",
    "# Create the manual DAG\n",
    "emotion_classification_manual_dag = create_manual_dag()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For local testing outside of Airflow\n",
    "    print(\"Testing pipeline submission...\")\n",
    "    try:\n",
    "        result = submit_pipeline()\n",
    "        print(f\"Pipeline submitted with ID: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-clf-pipeline-2SkgyElo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
